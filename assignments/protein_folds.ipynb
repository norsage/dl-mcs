{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Структурная классификация белков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотя аминокислотная последовательность однозначно определяет трёхмерную структуру белка, предсказание полной 3D-структуры является сложной задачей.\n",
    "\n",
    "В этом задании вам предстоит обучить несколько моделей для более простой задачи: предсказания типа белковой укладки по аминокислотной последовательности:\n",
    "\n",
    "<style>\n",
    "td, th {\n",
    "   border: none!important;\n",
    "}\n",
    "</style>\n",
    "| Orthogonal bundle | Beta-sandwich | Alpha-beta roll |\n",
    "| :-------: | :------: | :----: |\n",
    "|<img src=\"../assets/images/orthogonal_bundle.png\" width=\"300\"/>|<img src=\"../assets/images/beta_sandwich.png\" width=\"300\"/>|<img src=\"../assets/images/alpha_beta_roll.png\" width=\"300\"/>|\n",
    "\n",
    "\n",
    "Датасет получен на основе базы данных [CATH](https://www.cathdb.info/wiki?id=data:index), представляющей иерархическую и стандартизированную классификацию белковых доменов по типам укладки. В датасете сделана подвыборка 9 архитектур укладки белков, для каждой собрано 2k примеров небольшой длины последовательности.\n",
    "\n",
    "Данные и метки классов лежат в `assets/datasets/protein_fold`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс датасета уже написан:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CathSequencesDataset(Dataset):\n",
    "    def __init__(self, subset_csv: Path, labels_csv: Path, add_cls_token: bool = False) -> None:\n",
    "        # словарь: 20 аминокислот + 2 спец символа:\n",
    "        # _ - pad token, добивает последовательность до нужной длины\n",
    "        # ? — токен класса, он будет нужен для обучения трансформера\n",
    "        vocab = \"_?ACDEFGHIKLMNPQRSTVWY\"\n",
    "        df = pd.read_csv(subset_csv)\n",
    "        labels = pd.read_csv(labels_csv)[\"architecture\"]\n",
    "        self.add_cls_token = add_cls_token\n",
    "        self.vocab = {char: i for i, char in enumerate(vocab)}\n",
    "        self.label_dict = {name: i for i, name in enumerate(labels)}\n",
    "        self.sequences = [self._encode_sequence(s) for s in df[\"sequence\"]]\n",
    "        self.labels = [self.label_dict[label] for label in df[\"architecture\"]]\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[list[int], int]:\n",
    "        return self.sequences[index], self.labels[index]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def _encode_sequence(self, sequence: str) -> list[int]:\n",
    "        tokens = [self.vocab[char] for char in sequence]\n",
    "        if self.add_cls_token:\n",
    "            tokens = [1] + tokens\n",
    "        return tokens\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch: list[tuple[list[int], int]]) -> tuple[Tensor, Tensor]:\n",
    "        encoded, labels = zip(*batch)\n",
    "        max_len = max(map(len, encoded))\n",
    "        x = torch.zeros((len(encoded), max_len), dtype=int)\n",
    "        for i, seq in enumerate(encoded):\n",
    "            x[i, : len(seq)] = torch.tensor(seq)\n",
    "\n",
    "        return x, torch.tensor(list(labels))\n",
    "\n",
    "    def batch_decode(self, out_tokens: Tensor) -> list[str]:\n",
    "        # декодирование всего батча токенов\n",
    "        # сделано неточно, так как при получении токена окончания генерации (!)\n",
    "        # остаток последовательности нужно отбросить\n",
    "        inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        decoded_strings: list[str] = []\n",
    "        for x in out_tokens:\n",
    "            decoded_strings.append(\"\".join([inv_vocab[i] for i in x.tolist() if i > 2]))\n",
    "\n",
    "        return decoded_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID токенов: [14, 12, 19, 6, 11, 6, 14, 14, 10, 14, 10, 4, 18, 11, 12, 9, 17, 16, 18, 14, 5, 19, 18, 3, 19, 19, 19, 4, 19, 17, 4, 5, 4, 14, 5, 19, 10, 6, 13, 20, 21, 19, 4, 7, 19, 5, 19, 8, 13, 2, 10, 18, 10, 14, 16, 5, 5, 15, 21, 13, 2, 18, 21, 16, 19, 19, 17, 19, 11, 18, 19, 11, 8, 15, 4, 20, 11, 13, 7, 10, 5, 21, 10, 3, 10, 19, 17, 13, 10, 4, 11, 14, 2, 14, 9, 5, 10, 18, 9, 17]\n",
      "ID метки класса 4\n"
     ]
    }
   ],
   "source": [
    "rootdir = Path(\"../assets/datasets/protein_fold\")\n",
    "train_dataset = CathSequencesDataset(subset_csv=rootdir / \"train.csv\", labels_csv=rootdir / \"labels.csv\", add_cls_token=False)\n",
    "tokens, label = train_dataset[0]\n",
    "print(\"ID токенов:\", tokens)\n",
    "print(\"ID метки класса\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последовательности в датасете имеют различную длину, поэтому для их объединения в один батч в `DataLoader(..., collate_fn=)` нужно будет передать фукнцию, которая правильно упаковывает список наблюдений в тензоры. Подходящая функция реализована в методе `CathSequencesDataset.collate_fn`\n",
    "\n",
    "Посмотрим на мини-батч наблюдений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Последовательности: torch.Size([4, 129])\n",
      "Метки классов: torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[14, 12, 19,  6, 11,  6, 14, 14, 10, 14, 10,  4, 18, 11, 12,  9, 17, 16,\n",
       "         18, 14,  5, 19, 18,  3, 19, 19, 19,  4, 19, 17,  4,  5,  4, 14,  5, 19,\n",
       "         10,  6, 13, 20, 21, 19,  4,  7, 19,  5, 19,  8, 13,  2, 10, 18, 10, 14,\n",
       "         16,  5,  5, 15, 21, 13,  2, 18, 21, 16, 19, 19, 17, 19, 11, 18, 19, 11,\n",
       "          8, 15,  4, 20, 11, 13,  7, 10,  5, 21, 10,  3, 10, 19, 17, 13, 10,  4,\n",
       "         11, 14,  2, 14,  9,  5, 10, 18,  9, 17,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0],\n",
       "        [ 7, 14, 11,  7, 17, 14,  5,  6, 18, 17, 12, 10, 14,  9, 13,  5,  4, 11,\n",
       "          4,  5,  7,  9, 12, 19, 19, 21, 10, 16, 13,  9,  2,  7, 17,  7,  3, 15,\n",
       "         11, 18,  6, 20,  5,  2, 17,  5, 16, 18,  9, 16, 17,  5,  2,  5,  4, 17,\n",
       "         21,  8,  6, 17, 17,  2, 10, 12, 18,  2, 18,  6, 11, 17, 10, 10, 15,  5,\n",
       "         19, 13, 12, 17,  4, 17,  2, 11,  4,  3, 19, 16,  4,  5,  2,  9, 13, 10,\n",
       "         11, 15, 15,  9,  6, 13, 18, 17, 21, 13, 15, 18, 21,  5, 10, 21,  7, 13,\n",
       "         19, 17, 19,  6,  5, 18, 18,  7,  7, 11, 19, 19,  6, 20, 15,  7,  9, 10,\n",
       "         15, 10, 17],\n",
       "        [12, 18,  9, 17,  5, 11, 11, 15, 21,  3, 12,  2, 10, 14,  7,  2,  5, 15,\n",
       "         17, 19,  8, 13,  4, 20, 10,  2, 18, 15,  9, 10, 19,  5,  4, 19, 11,  6,\n",
       "          2, 12, 19, 10,  5, 19,  5, 13, 16, 14,  2, 19, 17, 11, 10, 18, 17, 14,\n",
       "          5, 11,  2,  5, 11, 11, 16, 15, 15,  8, 17,  4, 19, 16, 14, 17, 16,  8,\n",
       "         11, 13, 10,  2,  8, 20, 17, 18, 19, 21, 11,  4,  7, 17, 11, 14,  4, 17,\n",
       "         15,  9, 21, 21, 11, 19,  4,  2, 17, 21, 15, 15,  2, 19, 13, 11, 11, 14,\n",
       "          5,  5, 10, 16, 10, 11, 11, 19, 15, 11, 11,  5,  8,  8,  8,  8,  8,  8,\n",
       "          0,  0,  0],\n",
       "        [ 9, 17,  9, 11,  6,  6, 10, 13, 17, 13, 10, 10, 21, 10, 13, 18,  8, 17,\n",
       "         18,  6, 11, 17, 13,  6, 13,  9,  7, 19,  9, 10, 13, 18,  9, 21,  5,  4,\n",
       "          9, 11, 16, 11, 10, 13, 19, 13, 18,  9,  6, 11,  2,  4, 13, 17, 15,  5,\n",
       "         11, 19, 11,  2, 11, 10, 13,  4, 10, 19,  4, 21,  9, 21,  7,  4,  3, 10,\n",
       "         18, 11,  8, 21,  9,  2, 13, 13,  6, 11, 17,  5,  4, 11, 19,  9,  6, 18,\n",
       "          7,  4, 19,  6, 21, 17,  9, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_dataset = CathSequencesDataset(subset_csv=rootdir / \"train.csv\", labels_csv=rootdir / \"labels.csv\", add_cls_token=False)\n",
    "# sequences, labels = train_dataset.collate_fn([train_dataset[i] for i in range(4)])\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, collate_fn=train_dataset.collate_fn)\n",
    "sequences, labels = next(iter(train_loader))\n",
    "print(\"Последовательности:\", sequences.shape)\n",
    "print(\"Метки классов:\", labels.shape)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1 (5 баллов). Обучение классификатора на основе `nn.GRU`\n",
    "\n",
    "Реализуйте классификатор последовательностей на основе GRU.\n",
    "Ваш модуль будет иметь три нейросетевых блока:\n",
    "1. Получение эмбеддингов для токенов\n",
    "2. Получение скрытых состояний из рекуррентной сети\n",
    "3. Классификатор последовательности на основе последнего скрытого состояния\n",
    "\n",
    "В этом задании добейтесь точности классификации на валидационной выборке > 50%, для этого должно быть достаточно обучения в 15-20 эпох."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor, nn\n",
    "\n",
    "\n",
    "class GRUClassifier(nn.Module):\n",
    "    \"\"\"Модель для классификации\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, hidden_dim: int, n_classes: int) -> None:\n",
    "        super().__init__()\n",
    "        # эмбеддинги для аминокислот\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_dim, padding_idx=0)\n",
    "        # рекуррентный модуль\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,  # обратите внимание на этот параметр! Это частый источник ошибок при работе с рекуррентными сетями\n",
    "        )\n",
    "        # голова модели\n",
    "        self.classifier = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, tokens: Tensor) -> Tensor:\n",
    "        # B — размер батча\n",
    "        # L — длина последовательности\n",
    "        # H — размерность эмбеддингов\n",
    "        # C — число классов\n",
    "        # V — размер словаря токенов\n",
    "\n",
    "        # получаем эмбеддинги токенов: B x L -> B x L x H\n",
    "        x = ...\n",
    "        # обрабатываем последовательность токенов с помощью RNN: B x L x H -> B x L x H\n",
    "        ...\n",
    "        # извлекаем последние скрытые состояния: B x L x H -> B x H\n",
    "        last_state = ...\n",
    "\n",
    "        # используем их для классификации последовательностей: B x H -> B x C\n",
    "        logits = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2 (5 баллов). Обучение классификатора на основе TransformerEncoder\n",
    "\n",
    "Реализуйте классификатор последовательностей на основе блоков трансформера, приведённых в [notebooks/transformer_attention.ipynb](../notebooks/transformer_attention.ipynb).\n",
    "\n",
    "Ваш модуль будет иметь три нейросетевых блока:\n",
    "1. Получение начальных эмбеддингов для токенов и позиций\n",
    "2. Преобразование эмбеддингов последовательностью блоков трансформера\n",
    "3. Классификатор на основе финального эмбеддинга для токена <CLS>, который находится в начале каждой последовательности (не забудьте его включить в датасете!)\n",
    "\n",
    "В этом задании добейтесь точности классификации на валидационной выборке > 60%, для этого должно быть достаточно обучения в 15-20 эпох."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_size: int, n_layers: int, n_classes: int = 9, max_position: int = 300):\n",
    "        super().__init__()\n",
    "        self.embeds = ...  # эмбеддинги токенов + позиций\n",
    "        self.trunc = ...  # последовательность SelfAttention модулей\n",
    "        self.classifier = ...  # классификатор\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3 (2 балла). Pre-norm vs Post-norm\n",
    "\n",
    "Измените порядок следования механизма внимания, MLP и нормализаций на схему pre-normalized активаций (см. слайд **Постпроцессинг токенов: нормализация и MLP** в [slides/rnn_attention.pdf](../slides/rnn_attention.pdf)).\n",
    "\n",
    "Запустите эксперимент с вашей лучшей конфигурацией модели из задания 2, заменив только определение `TransformerLayer`.\n",
    "\n",
    "Изменилась ли точность или динамика обучения?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
