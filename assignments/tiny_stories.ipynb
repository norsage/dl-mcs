{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1cabe0b",
   "metadata": {},
   "source": [
    "### TinyStories: генерация коротких текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898aa1a4",
   "metadata": {},
   "source": [
    "В этом задании вы реализуете transformer decoder и обучите его на датасете TinyStories, а также продемонстрируете его работу, сгенерировав несколько примеров текстов по заданному началу.\n",
    "\n",
    "Большая часть блоков уже описана в слайдах и в ноутбуках [notebooks/transformer_attention.ipynb](../notebooks/transformer_attention.ipynb) и [notebooks/transformer_model.ipynb](../notebooks/transformer_model.ipynb), вам осталось только собрать их в модуль `TransformerDecoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43c53b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5f4bd3",
   "metadata": {},
   "source": [
    "Загрузим датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "158e5fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2119719\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 21990\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"roneneldan/TinyStories\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322d3bda",
   "metadata": {},
   "source": [
    "Посмотрим на несколько примеров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e999dfb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.',\n",
       " 'Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.\\n\\nOne day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were falling. Beep liked how the leaves fall and wanted to play with them. Beep drove under the tree and watched the leaves fall on him. He laughed and beeped his horn.\\n\\nBeep played with the falling leaves all day. When it was time to go home, Beep knew he needed more fuel. He went to the fuel place and got more healthy fuel. Now, Beep was ready to go fast and play again the next day. And Beep lived happily ever after.',\n",
       " 'One day, a little fish named Fin was swimming near the shore. He saw a big crab and wanted to be friends. \"Hi, I am Fin. Do you want to play?\" asked the little fish. The crab looked at Fin and said, \"No, I don\\'t want to play. I am cold and I don\\'t feel fine.\"\\n\\nFin felt sad but wanted to help the crab feel better. He swam away and thought of a plan. He remembered that the sun could make things warm. So, Fin swam to the top of the water and called to the sun, \"Please, sun, help my new friend feel fine and not freeze!\"\\n\\nThe sun heard Fin\\'s call and shone its warm light on the shore. The crab started to feel better and not so cold. He saw Fin and said, \"Thank you, little fish, for making me feel fine. I don\\'t feel like I will freeze now. Let\\'s play together!\" And so, Fin and the crab played and became good friends.',\n",
       " 'Once upon a time, in a land full of trees, there was a little cherry tree. The cherry tree was very sad because it did not have any friends. All the other trees were big and strong, but the cherry tree was small and weak. The cherry tree was envious of the big trees.\\n\\nOne day, the cherry tree felt a tickle in its branches. It was a little spring wind. The wind told the cherry tree not to be sad. The wind said, \"You are special because you have sweet cherries that everyone loves.\" The cherry tree started to feel a little better.\\n\\nAs time went on, the cherry tree grew more and more cherries. All the animals in the land came to eat the cherries and play under the cherry tree. The cherry tree was happy because it had many friends now. The cherry tree learned that being different can be a good thing. And they all lived happily ever after.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][\"text\"][:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404d5e54",
   "metadata": {},
   "source": [
    "Будем использовать готовый токенизатор, например `GPT2TokenizerFast`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9feb4178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3f18802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7454, 2402,  257,  640]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"Once upon a time\", return_tensors=\"pt\", add_special_tokens=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd98854",
   "metadata": {},
   "source": [
    "У этого токенизатора нет выделенного токена для начала последовательности, есть только один специальный токен `<|endoftext|>`. Поэтому, если вы захотите добавить его к началу предложения, придётся делать это вручную:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ab77ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256,  7454,  2402,   257,   640]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|endoftext|>Once upon a time\", return_tensors=\"pt\", add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b6e684",
   "metadata": {},
   "source": [
    "Вместо этого можно добавлять его индекс `50256` в начало всех тензоров уже после токенизации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c15e00",
   "metadata": {},
   "source": [
    "Токенизатор можно использовать прямо в загрузчике данных для упаковки токенизированных последовательностей в батчи.\n",
    "\n",
    "Помимо индексов токенов (`input_ids`) токенизатор вернёт нам `attention_mask` — маску, в которой нулями помечены `pad` токены, использующиеся для выравнивания предложений по длине."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04a0c0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 3198,  1110,    11,  ..., 50256, 50256, 50256],\n",
       "        [ 7454,  2402,   257,  ..., 50256, 50256, 50256],\n",
       "        [ 3198,  1110,    11,  ...,   922,  2460,    13],\n",
       "        ...,\n",
       "        [ 7454,  2402,   257,  ..., 50256, 50256, 50256],\n",
       "        [ 7454,  2402,   257,  ..., 50256, 50256, 50256],\n",
       "        [ 7454,  2402,   257,  ..., 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DataLoader(\n",
    "    dataset=ds[\"train\"][\"text\"],\n",
    "    collate_fn=lambda batch: tokenizer(batch, return_tensors=\"pt\", padding=True),\n",
    "    batch_size=8,\n",
    ")\n",
    "batch = next(iter(loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0377fe5",
   "metadata": {},
   "source": [
    "#### Задание 1. Обучение Transformer Decoder (4 балла)\n",
    "\n",
    "1. Реализуйте модуль `TransformerDecoder`. Не забудьте сформировать правильную маску внимания! Для этого можете воспользоваться функцией `create_causal_mask` ниже.\n",
    "2. Создайте с его помощью модель небольшого размера (до 30 млн параметров) и обучите её на `train` части датасета TinyStories. Обучение на всём датасете может занять слишком много времени, так что сделайте столько итераций градиентного спуска, сколько уместится в 10-15 минут обучения. Кроме того, вы можете сделать подвыборку датасета с более короткими текстами, это ускорит обучение модели. Относительно связный текст обычно начинает получаться при снижении cross entropy до 1.5-2.0.\n",
    "3. После окончания обучения сгенерируйте 10 историй с помощью вашей модели.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ff18a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(token_ids: Tensor, pad_token_id: int = 0) -> Tensor:\n",
    "    B, T = token_ids.shape\n",
    "    # маска для <pad> токенов\n",
    "    pad_mask = (token_ids != pad_token_id).unsqueeze(1)  # B x 1 x T\n",
    "    # маска нижнетреугольной матрицы\n",
    "    causal_mask = torch.tril(\n",
    "        torch.ones((T, T), device=token_ids.device)\n",
    "    ).bool()  # T x T\n",
    "    return pad_mask & causal_mask  # B x T x T\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_size: int, n_layers: int):\n",
    "        ...\n",
    "\n",
    "    def forward(self, input_ids: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c886d026",
   "metadata": {},
   "source": [
    "Для генерации примеров можно использовать простейшую стратегию с семплированием следующего токена:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2925e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model: TransformerDecoder, input_ids: Tensor, max_new_tokens: int = 200\n",
    ") -> Tensor:\n",
    "    for t in range(max_new_tokens):\n",
    "        mask = create_causal_mask(input_ids)\n",
    "        logits = model.forward(input_ids, mask)[:, -1]\n",
    "        # new_token = logits.argmax(dim=-1, keepdim=True)\n",
    "        new_token = torch.multinomial(logits.softmax(-1), num_samples=1)\n",
    "        input_ids = torch.cat([input_ids, new_token], dim=1)\n",
    "\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c30dd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Once upon a time\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device=\"mps\")\n",
    "generate_ids = generate(model, inputs.input_ids, max_new_tokens=200)\n",
    "response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
